{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse fields in (cleaned) updated data\n",
    "Author: Daheng Wang  \n",
    "Last modified: 20170425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Map\n",
    "1. Parse necessary fields in cleaned updated data\n",
    "2. Build a new collection of unique users information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import codecs\n",
    "import os\n",
    "import json\n",
    "from pymongo import IndexModel, ASCENDING, DESCENDING\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "import multiprocessing\n",
    "import logging\n",
    "import datetime\n",
    "import shelve\n",
    "\n",
    "import mongodb # module for setting up connection with (local) MongoDB database\n",
    "import multiprocessing_workers # module for splitting workloads between processes\n",
    "import utilities # module for various custom utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = 'tweets_ek' # database for tweets collected on expanded keywords\n",
    "RAW_COLLECTION_NAME = 'c1' # collection for raw data\n",
    "UPDATED_COLLECTION_NAME = 'c2' # collection for updated data\n",
    "\n",
    "updated_data = mongodb.initialize(db_name=DB_NAME, collection_name=UPDATED_COLLECTION_NAME)\n",
    "\n",
    "# en_updated_data = mongodb.initialize(db_name=DB_NAME, collection_name=EN_UPDATED_COLLETION_NAME)\n",
    "# nonen_updated_data = mongodb.initialize(db_name=DB_NAME, collection_name=NONEN_UPDATED_COLLETION_NAME)\n",
    "# db = mongodb.initialize_db(db_name=DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse necesary fields in cleaned updated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse 'created_at' field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'created_at' filed of tweets received from Twitter API contains a fixed format of string representing the datatime information  \n",
    "Example: ```Tue Feb 07 04:59:37 +0000 2017```  \n",
    "This default string representation of datetime cannot be efficiently processed by MongoDB database, especially in aggregation operations. We parse it into a universal datetime representation format: Unix timestamp (in milliseconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Step 1_ check how many tweets have 'timestamp_ms' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    total_n = updated_data.count()\n",
    "    %time timestamp_ms_n = updated_data.count(filter={'timestamp_ms': {'$exists': True}})\n",
    "    print(\"Tweets with 'timestamp_ms' field: {} ({})\".format(timestamp_ms_n, timestamp_ms_n / total_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE Tweets received from Streaming API contain 'timestamp_ms' field. Tweets queried from REST API do not have 'timestamp_ms' field.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Step 2_ use multiple ~~threads~~ (see http://www.dabeaz.com/GIL/ and https://jeffknupp.com/blog/2013/06/30/pythons-hardest-problem-revisited/ for Python GIL problem) **processes** to compute concurrently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, worker functions are wrapped in multiprocessing_workers.py file (see https://pymotw.com/3/multiprocessing/basics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-0/11] child process calling self.run()\n",
      "[INFO/Process-1/11] child process calling self.run()\n",
      "[INFO/Process-2/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-3/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process0/11 handling documents 0 to 458506...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-4/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-5/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-6/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-7/11] child process calling self.run()\n",
      "[INFO/Process-8/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-9/11] child process calling self.run()\n",
      "[INFO/Process-10/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "Process1/11 handling documents 458507 to 917013...\n",
      "Process2/11 handling documents 917014 to 1375520...\n",
      "Process3/11 handling documents 1375521 to 1834027...\n",
      "Process4/11 handling documents 1834028 to 2292534...\n",
      "Process5/11 handling documents 2292535 to 2751041...\n",
      "Process6/11 handling documents 2751042 to 3209548...\n",
      "Process7/11 handling documents 3209549 to 3668055...\n",
      "Process8/11 handling documents 3668056 to 4126562...\n",
      "Process9/11 handling documents 4126563 to 4585069...\n",
      "Process10/11 handling documents 4585070 to 5043587...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-0/11] process shutting down\n",
      "[DEBUG/Process-0/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-0/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-0/11] process exiting with exitcode 0\n",
      "[INFO/Process-1/11] process shutting down\n",
      "[DEBUG/Process-1/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-1/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-1/11] process exiting with exitcode 0\n",
      "[INFO/Process-2/11] process shutting down\n",
      "[DEBUG/Process-2/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-2/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-2/11] process exiting with exitcode 0\n",
      "[INFO/Process-3/11] process shutting down\n",
      "[DEBUG/Process-3/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-3/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-3/11] process exiting with exitcode 0\n",
      "[INFO/Process-4/11] process shutting down\n",
      "[DEBUG/Process-4/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-4/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-4/11] process exiting with exitcode 0\n",
      "[INFO/Process-5/11] process shutting down\n",
      "[DEBUG/Process-5/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-5/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-5/11] process exiting with exitcode 0\n",
      "[INFO/Process-6/11] process shutting down\n",
      "[DEBUG/Process-6/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-6/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-6/11] process exiting with exitcode 0\n",
      "[INFO/Process-7/11] process shutting down\n",
      "[DEBUG/Process-7/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-7/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-7/11] process exiting with exitcode 0\n",
      "[INFO/Process-8/11] process shutting down\n",
      "[DEBUG/Process-8/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-8/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-8/11] process exiting with exitcode 0\n",
      "[INFO/Process-9/11] process shutting down\n",
      "[DEBUG/Process-9/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-9/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-9/11] process exiting with exitcode 0\n",
      "[INFO/Process-10/11] process shutting down\n",
      "[DEBUG/Process-10/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-10/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-10/11] process exiting with exitcode 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 92 ms, total: 188 ms\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inter_files = []\n",
    "if 0 == 1:\n",
    "    procedure_name = 'parse_{}_created_at'.format(UPDATED_COLLECTION_NAME)\n",
    "    \n",
    "    multiprocessing.log_to_stderr(logging.DEBUG)\n",
    "    process_n = multiprocessing.cpu_count() - 1 # set processes number to CPU numbers minus 1\n",
    "    suffix = 'json'\n",
    "    inter_files = utilities.gen_inter_filenames_list(procedure_name, process_n, suffix)\n",
    "    \n",
    "    jobs = []\n",
    "    for batch_i in range(process_n):\n",
    "        p = multiprocessing.Process(target=multiprocessing_workers.worker_parse_created_at,\n",
    "                                    args=(DB_NAME, UPDATED_COLLECTION_NAME, batch_i, process_n, inter_files[batch_i]),\n",
    "                                    name='Process-{}/{}'.format(batch_i, process_n))\n",
    "        jobs.append(p)\n",
    "    \n",
    "    for job in jobs:\n",
    "        job.start()\n",
    "        \n",
    "    for job in jobs:\n",
    "        job.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Step 3_ Import all parsed data back into database new collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2_parsed_created_at connected successfully!\n",
      "Reading inter/parse_c2_created_at-0.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-1.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-2.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-3.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-4.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-5.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-6.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-7.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-8.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-9.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Reading inter/parse_c2_created_at-10.json...\n",
      "Importing into tweets_ek.c2_parsed_created_at\n",
      "Done\n",
      "CPU times: user 3min 55s, sys: 2.16 s, total: 3min 57s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PARSED_CREATED_AT_COLLECTION = 'c2_parsed_created_at'\n",
    "if 0 == 1:\n",
    "    parsed_created_at_col = mongodb.initialize(db_name=DB_NAME, collection_name=PARSED_CREATED_AT_COLLECTION)\n",
    "    for inter_file in inter_files:\n",
    "        print('Reading {}...'.format(inter_file), end=' ')\n",
    "        lines = open(inter_file).readlines()\n",
    "        parsed_jsons = [json.loads(line) for line in lines]\n",
    "        \n",
    "        # it's important to reconstruct datetime.datetime obj back\n",
    "        # otherwise, the 'created_at_parsed' field cannot be imported into MongoDB\n",
    "        # http://api.mongodb.com/python/1.3/tutorial.html\n",
    "        reconstructed_jsons = [{'id': int(parsed_json['id']), \n",
    "                               'created_at_parsed': datetime.datetime.fromtimestamp(parsed_json['created_at_parsed'])} \n",
    "                              for parsed_json in parsed_jsons]\n",
    "        print('Importing into {}.{}...'.format(DB_NAME, PARSED_CREATED_AT_COLLECTION))\n",
    "        parsed_created_at_col.insert_many(reconstructed_jsons)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new collection size and print a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2_parsed_created_at connected successfully!\n",
      "Collection c2_parsed_created_at size: 5043587\n",
      "Sample document:\n",
      "{'_id': ObjectId('58fe568cfe57a16f94c04911'),\n",
      " 'created_at_parsed': datetime.datetime(2017, 3, 6, 1, 8, 5),\n",
      " 'id': 838632256252088320}\n"
     ]
    }
   ],
   "source": [
    "if 0 == 1:\n",
    "    parsed_created_at_col = mongodb.initialize(db_name=DB_NAME, collection_name=PARSED_CREATED_AT_COLLECTION)\n",
    "    print('Collection {} size: {}'.format(PARSED_CREATED_AT_COLLECTION, parsed_created_at_col.count()))\n",
    "    print('Sample document:')\n",
    "    pprint(parsed_created_at_col.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Parse 'user.description' field for keyword 'ibm'\n",
    "Test whether the keyword 'ibm' exists in the 'user.description' field. If yes, we consider the user shows explicit affiliation with IBM; if no, we do not know if the user is affliated with IBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse 'root.text' field for different topics (keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a new collection of unique users information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Step 1_ Get a set of unique user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_user_ids_shl = os.path.join('data', 'unique_user_ids.db')\n",
    "unique_user_ids_key = 'unique_user_ids'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying MongoDB for unique user ids...\n",
      "Building unique user ids set from list...\n",
      "Writing out user ids set to shelve data/unique_user_ids.db...\n",
      "Done\n",
      "CPU times: user 1min, sys: 1.12 s, total: 1min 1s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "if 0 == 1:\n",
    "    print('Querying MongoDB for unique user ids...')\n",
    "    unique_user_ids_int64_list = []\n",
    "    cursor = updated_data.find(projection={'_id': 0, 'user.id': 1})\n",
    "    for document in cursor:\n",
    "        user_id_int64 = int(document['user']['id'])\n",
    "        unique_user_ids_int64_list.append(user_id_int64)\n",
    "    \n",
    "    print('Building unique user ids set from list...')\n",
    "    unique_user_ids_int64_set = set(unique_user_ids_int64_list)\n",
    "    \n",
    "    # write out to shelve\n",
    "    print('Writing out user ids set to shelve {} size {}'.format(unique_user_ids_shl, len(unique_user_ids_int64_set)))\n",
    "    with shelve.open(unique_user_ids_shl) as s:\n",
    "        s[unique_user_ids_key] = unique_user_ids_int64_set # store data at key (overwrites old data if using an existing key)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Step 2_ For each unique user id in the set, (multiprocessing) query database and write out to intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "Process0/11 querying users 0 to 76788...\n",
      "Process1/11 querying users 76788 to 153576...\n",
      "Process2/11 querying users 153576 to 230364...\n",
      "Process3/11 querying users 230364 to 307152...\n",
      "Process4/11 querying users 307152 to 383940...\n",
      "Process5/11 querying users 383940 to 460728...\n",
      "Process6/11 querying users 460728 to 537516...\n",
      "Process7/11 querying users 537516 to 614304...\n",
      "Process8/11 querying users 614304 to 691092...\n",
      "Process9/11 querying users 691092 to 767880...\n",
      "Process10/11 querying users 767880 to 844675...\n",
      "Process0/11 Done\n",
      "Process10/11 Done\n",
      "Process9/11 Done\n",
      "Process8/11 Done\n",
      "Process7/11 Done\n",
      "Process6/11 Done\n",
      "Process5/11 Done\n",
      "Process4/11 Done\n",
      "Process2/11 Done\n",
      "Process3/11 Done\n",
      "Process1/11 Done\n",
      "CPU times: user 60 ms, sys: 116 ms, total: 176 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inter_files = []\n",
    "if 0 == 1:\n",
    "    # generate intermediate filenames\n",
    "    procedure_name = 'get_{}_unique_user_ids'.format(UPDATED_COLLECTION_NAME)\n",
    "    \n",
    "    process_n = multiprocessing.cpu_count() - 1 # set processes number to CPU numbers minus 1\n",
    "    suffix = 'json'\n",
    "    inter_files = utilities.gen_inter_filenames_list(procedure_name, process_n, suffix)\n",
    "    \n",
    "    jobs = []\n",
    "    for batch_i in range(process_n):\n",
    "        p = multiprocessing.Process(target=multiprocessing_workers.worker_get_unique_user,\n",
    "                                    args=(DB_NAME, UPDATED_COLLECTION_NAME,\n",
    "                                          batch_i, process_n, inter_files[batch_i],\n",
    "                                          unique_user_ids_shl, unique_user_ids_key),\n",
    "                                    name='Process-{}/{}'.format(batch_i, process_n))\n",
    "        jobs.append(p)\n",
    "    \n",
    "    for job in jobs:\n",
    "        job.start()\n",
    "        \n",
    "    for job in jobs:\n",
    "        job.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Step 3_ Import all unique user data into database new collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2_users connected successfully!\n",
      "Reading inter/get_c2_unique_user_ids-0.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-1.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-2.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-3.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-4.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-5.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-6.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-7.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-8.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-9.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-10.json... Importing into tweets_ek.c2_users...\n",
      "Done\n",
      "CPU times: user 2min 29s, sys: 6.82 s, total: 2min 36s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "USERS_COLLECTION = 'c2_users'\n",
    "if 0 == 1:\n",
    "    user_col = mongodb.initialize(db_name=DB_NAME, collection_name=USERS_COLLECTION)\n",
    "    for inter_file in inter_files:\n",
    "        print('Reading {}...'.format(inter_file), end=' ')\n",
    "        parsed_jsons = []\n",
    "        with open(inter_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parsed_jsons.append(json.loads(line))\n",
    "        print('Importing into {}.{}...'.format(DB_NAME, USERS_COLLECTION))\n",
    "        user_col.insert_many(parsed_jsons)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new collection size and print a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2_users connected successfully!\n",
      "Collection c2_users size: 844675\n",
      "Sample document:\n",
      "{'_id': ObjectId('58fed783fe57a10b2393c51e'),\n",
      " 'contributors_enabled': False,\n",
      " 'created_at': 'Tue Mar 21 20:50:14 +0000 2006',\n",
      " 'default_profile': False,\n",
      " 'default_profile_image': False,\n",
      " 'description': '',\n",
      " 'entities': {'description': {'urls': []}},\n",
      " 'favourites_count': 16835,\n",
      " 'follow_request_sent': False,\n",
      " 'followers_count': 4028041,\n",
      " 'following': False,\n",
      " 'friends_count': 2677,\n",
      " 'geo_enabled': True,\n",
      " 'has_extended_profile': True,\n",
      " 'id': 12,\n",
      " 'id_str': '12',\n",
      " 'is_translation_enabled': False,\n",
      " 'is_translator': False,\n",
      " 'lang': 'en',\n",
      " 'listed_count': 27165,\n",
      " 'location': 'California, USA',\n",
      " 'name': 'jack',\n",
      " 'notifications': False,\n",
      " 'profile_background_color': 'EBEBEB',\n",
      " 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme7/bg.gif',\n",
      " 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme7/bg.gif',\n",
      " 'profile_background_tile': False,\n",
      " 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/12/1483046077',\n",
      " 'profile_image_url': 'http://pbs.twimg.com/profile_images/839863609345794048/mkpdB9Tf_normal.jpg',\n",
      " 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/839863609345794048/mkpdB9Tf_normal.jpg',\n",
      " 'profile_link_color': '990000',\n",
      " 'profile_sidebar_border_color': 'DFDFDF',\n",
      " 'profile_sidebar_fill_color': 'F3F3F3',\n",
      " 'profile_text_color': '333333',\n",
      " 'profile_use_background_image': True,\n",
      " 'protected': False,\n",
      " 'screen_name': 'jack',\n",
      " 'statuses_count': 21755,\n",
      " 'time_zone': 'Pacific Time (US & Canada)',\n",
      " 'translator_type': 'regular',\n",
      " 'url': None,\n",
      " 'utc_offset': -25200,\n",
      " 'verified': True}\n"
     ]
    }
   ],
   "source": [
    "if 0 == 1:\n",
    "    user_col = mongodb.initialize(db_name=DB_NAME, collection_name=USERS_COLLECTION)\n",
    "    print('Collection {} size: {}'.format(USERS_COLLECTION, user_col.count()))\n",
    "    print('Sample document:')\n",
    "    pprint(user_col.find_one())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
