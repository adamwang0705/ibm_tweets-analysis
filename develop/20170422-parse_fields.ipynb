{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Parse fields\n",
    "Author: Daheng Wang  \n",
    "Last modified: 2017-05-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Road Map\n",
    "1. Parse 'created_at'/'timestamp_ms' filed for the creation time of all tweets\n",
    "2. Tag 'text' filed of all tweets for different topics/keywords\n",
    "3. LEGACY Build new collectionS of unique users information (multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Standard modules, MongoDB modules\n",
    "'''\n",
    "import os, sys, json, datetime, pickle, multiprocessing, logging\n",
    "from pprint import pprint\n",
    "\n",
    "import pymongo\n",
    "from pymongo import IndexModel, ASCENDING, DESCENDING\n",
    "\n",
    "'''\n",
    "Custom tool modules\n",
    "'''\n",
    "import mongodb  # module for setting up connection with (local) MongoDB database\n",
    "import multiprocessing_workers  # module for splitting workloads between processes\n",
    "import utilities  # module for various custom utility functions\n",
    "from config import * # import all global configuration variables\n",
    "\n",
    "NB_NAME = '20170422-parse_fields'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Parse 'created_at'/'timestamp_ms' filed for the creation time of all tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The 'created_at' filed of tweets received from Twitter API contains a fixed format of string representing the datatime information. Example: ```Tue Feb 07 04:59:37 +0000 2017```  \n",
    "This default string representation of datetime cannot be efficiently processed by MongoDB database, especially in aggregation operations.  \n",
    "\n",
    "**NOTE**: Tweets received from Streaming API contain a 'timestamp_ms' field (Tweets queried from RESTful API do not have this field).  \n",
    "\n",
    "We want to parse out the standard Unix timestamp (in seconds) of each tweet, either by using 'created_at' filed or by using 'timestamp_ms' field.\n",
    "However, we prefer to use the 'timestamp_ms' field if it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": true,
     "read_only": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "Total num of raw tweets: 11635450\n",
      "Tweets with 'timestamp_ms' field: 11635450 (1.0)\n",
      "CPU times: user 192 ms, sys: 76 ms, total: 268 ms\n",
      "Wall time: 11min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Check how many tweets have 'timestamp_ms' field.\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    tw_raw_col = mongodb.initialize(DB_NAME, TW_RAW_COL)\n",
    "    tw_raw_num = tw_raw_col.count()\n",
    "    print(\"Total num of raw tweets: {}\".format(tw_raw_num))\n",
    "    timestamp_ms_num = tw_raw_col.count(filter={'timestamp_ms': {'$exists': True}})\n",
    "    print(\"Tweets with 'timestamp_ms' field: {} ({:.2%})\".format(timestamp_ms_num, timestamp_ms_num / tw_raw_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Parse the 'timestamp_ms' filed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the list of creation time...\n",
      "MongoDB on localhost:27017/tweets_ek-2 connected successfully!\n",
      "List length: 11635450\n",
      "Building new collection...\n",
      "Collection size: 11635450\n",
      "CPU times: user 10min 55s, sys: 17.5 s, total: 11min 12s\n",
      "Wall time: 23min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Build a new collection of parsed creation time for all raw tweets\n",
    "Register in config:\n",
    "    TW_RAW_PCT_COL = 'tw_raw_pct'\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    '''\n",
    "    Parse the creation time based on the 'timestamp_ms' filed\n",
    "    '''\n",
    "    print('Parsing the list of creation time...')\n",
    "    data_lst = []\n",
    "    \n",
    "    db = mongodb.initialize_db(db_name=DB_NAME)\n",
    "    tw_raw_col = db[TW_RAW_COL]\n",
    "    \n",
    "    cursor = tw_raw_col.find(projection={'_id': 0, 'id': 1, 'timestamp_ms': 1})\n",
    "    \n",
    "    for doc in cursor:\n",
    "        id_int = int(doc['id'])\n",
    "        timestamp_ms = int(doc['timestamp_ms'])\n",
    "        \n",
    "        timestamp = timestamp_ms // 1000 # parse into Unix timestamp\n",
    "        X_creation = datetime.datetime.fromtimestamp(timestamp) # parse into datetime obj\n",
    "        \n",
    "        item_dict = {'id': id_int, 'timestamp_ms': timestamp_ms, 'X_creation': X_creation}\n",
    "        data_lst.append(item_dict)\n",
    "    print('List length: {}'.format(len(data_lst)))\n",
    "    \n",
    "    '''\n",
    "    Insert into new collection\n",
    "    '''\n",
    "    print('Building new collection...')\n",
    "    db[TW_RAW_PCT_COL].insert_many(data_lst)\n",
    "    count = db[TW_RAW_PCT_COL].count()\n",
    "    print('Collection size: {}'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### LEGACY Parse the 'created_at' filed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Use multiple ~~threads~~ (see http://www.dabeaz.com/GIL/ and https://jeffknupp.com/blog/2013/06/30/pythons-hardest-problem-revisited/ for Python GIL problem) **processes** to compute concurrently. Worker function is wrapped in multiprocessing_workers.py (see https://pymotw.com/3/multiprocessing/basics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use multiprocessing to parse the 'created_at' field of all tweets.\n",
    "Worker function 'worker_parse_created_at' is wrapped in multiprocessing_workers.py.\n",
    "\"\"\"\n",
    "inter_files = []\n",
    "if 0 == 1:\n",
    "    procedure_name = 'parse_{}_created_at'.format(UPDATED_COL)\n",
    "    \n",
    "    multiprocessing.log_to_stderr(logging.DEBUG)\n",
    "    process_n = multiprocessing.cpu_count() - 1 # set processes number to CPU numbers minus 1\n",
    "    suffix = 'json'\n",
    "    inter_files = utilities.gen_inter_filenames_list(procedure_name, process_n, suffix)\n",
    "    \n",
    "    jobs = []\n",
    "    for batch_i in range(process_n):\n",
    "        p = multiprocessing.Process(target=multiprocessing_workers.worker_parse_created_at,\n",
    "                                    args=(DB_NAME, UPDATED_COL, batch_i, process_n, inter_files[batch_i]),\n",
    "                                    name='Process-{}/{}'.format(batch_i, process_n))\n",
    "        jobs.append(p)\n",
    "    \n",
    "    for job in jobs:\n",
    "        job.start()\n",
    "        \n",
    "    for job in jobs:\n",
    "        job.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a new collection for parsed 'created_at' field of tweets.\n",
    "Register in config:\n",
    "    PARSED_CREATED_AT_COL = 'c2_parsed_created_at'\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    parsed_created_at_col = mongodb.initialize(db_name=DB_NAME, collection_name=PARSED_CREATED_AT_COL)\n",
    "    for inter_file in inter_files:\n",
    "        print('Reading {}...'.format(inter_file), end=' ')\n",
    "        lines = open(inter_file).readlines()\n",
    "        parsed_jsons = [json.loads(line) for line in lines]\n",
    "        \n",
    "        # it's important to reconstruct datetime.datetime obj back\n",
    "        # otherwise, the 'created_at_parsed' field cannot be imported into MongoDB\n",
    "        # http://api.mongodb.com/python/1.3/tutorial.html\n",
    "        reconstructed_jsons = [{'id': int(parsed_json['id']), \n",
    "                               'created_at_parsed': datetime.datetime.fromtimestamp(parsed_json['created_at_parsed'])} \n",
    "                              for parsed_json in parsed_jsons]\n",
    "        print('Importing into {}.{}...'.format(DB_NAME, PARSED_CREATED_AT_COL))\n",
    "        parsed_created_at_col.insert_many(reconstructed_jsons)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check the new collection size and print a sample\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    parsed_created_at_col = mongodb.initialize(db_name=DB_NAME, collection_name=PARSED_CREATED_AT_COL)\n",
    "    print('Collection {} size: {}'.format(PARSED_CREATED_AT_COL, parsed_created_at_col.count()))\n",
    "    print('Sample document:')\n",
    "    pprint(parsed_created_at_col.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Tag 'text' filed of all tweets for different topics/keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": true,
     "read_only": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-4/11] child process calling self.run()\n",
      "[INFO/Process-5/11] child process calling self.run()\n",
      "[INFO/Process-0/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "Process0/11 handling documents 0 to 1057767...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-3/11] child process calling self.run()\n",
      "[INFO/Process-6/11] child process calling self.run()\n",
      "[INFO/Process-1/11] child process calling self.run()\n",
      "[INFO/Process-2/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-7/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-8/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-9/11] child process calling self.run()\n",
      "[INFO/Process-10/11] child process calling self.run()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw connected successfully!\n",
      "Process1/11 handling documents 1057768 to 2115535...\n",
      "Process2/11 handling documents 2115536 to 3173303...\n",
      "Process3/11 handling documents 3173304 to 4231071...\n",
      "Process4/11 handling documents 4231072 to 5288839...\n",
      "Process5/11 handling documents 5288840 to 6346607...\n",
      "Process6/11 handling documents 6346608 to 7404375...\n",
      "Process7/11 handling documents 7404376 to 8462143...\n",
      "Process8/11 handling documents 8462144 to 9519911...\n",
      "Process9/11 handling documents 9519912 to 10577679...\n",
      "Process10/11 handling documents 10577680 to 11635450...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO/Process-0/11] process shutting down\n",
      "[INFO/Process-0/11] process exiting with exitcode 0\n",
      "[DEBUG/Process-0/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-0/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-1/11] process shutting down\n",
      "[DEBUG/Process-1/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-1/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-1/11] process exiting with exitcode 0\n",
      "[INFO/Process-2/11] process shutting down\n",
      "[DEBUG/Process-2/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-2/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-2/11] process exiting with exitcode 0\n",
      "[INFO/Process-3/11] process shutting down\n",
      "[DEBUG/Process-3/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-3/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-3/11] process exiting with exitcode 0\n",
      "[INFO/Process-4/11] process shutting down\n",
      "[DEBUG/Process-4/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-4/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-4/11] process exiting with exitcode 0\n",
      "[INFO/Process-5/11] process shutting down\n",
      "[DEBUG/Process-5/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-5/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-5/11] process exiting with exitcode 0\n",
      "[INFO/Process-6/11] process shutting down\n",
      "[DEBUG/Process-6/11] running the remaining \"atexit\" finalizers\n",
      "[DEBUG/Process-6/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[INFO/Process-6/11] process exiting with exitcode 0\n",
      "[INFO/Process-7/11] process shutting down\n",
      "[INFO/Process-7/11] process exiting with exitcode 0\n",
      "[DEBUG/Process-7/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-7/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-8/11] process shutting down\n",
      "[DEBUG/Process-8/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-8/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-8/11] process exiting with exitcode 0\n",
      "[INFO/Process-9/11] process shutting down\n",
      "[DEBUG/Process-9/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-9/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-9/11] process exiting with exitcode 0\n",
      "[INFO/Process-10/11] process shutting down\n",
      "[DEBUG/Process-10/11] running all \"atexit\" finalizers with priority >= 0\n",
      "[DEBUG/Process-10/11] running the remaining \"atexit\" finalizers\n",
      "[INFO/Process-10/11] process exiting with exitcode 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 112 ms, sys: 96 ms, total: 208 ms\n",
      "Wall time: 17min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Use multiprocessing to tag the 'text' field of all tweets for filtering keywords passed to Twitter API\n",
    "Worker function 'worker_tag_kws_in_tw' is wrapped in multiprocessing_workers.py.\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    procedure_name = 'tag_{}_text'.format(TW_RAW_COL)\n",
    "    multiprocessing.log_to_stderr(logging.DEBUG)\n",
    "    process_n = multiprocessing.cpu_count() - 1 # set processes number to CPU numbers minus 1\n",
    "    suffix = 'json'\n",
    "    inter_files = utilities.gen_inter_filenames_list(NB_NAME, procedure_name, process_n, suffix)\n",
    "    \n",
    "    jobs = []\n",
    "    for batch_i in range(process_n):\n",
    "        p = multiprocessing.Process(target=multiprocessing_workers.worker_tag_kws_in_tw,\n",
    "                                    args=(DB_NAME, TW_RAW_COL, batch_i, process_n, inter_files[batch_i], API_QUERY_KWS),\n",
    "                                    name='Process-{}/{}'.format(batch_i, process_n))\n",
    "        jobs.append(p)\n",
    "    \n",
    "    for job in jobs:\n",
    "        job.start()\n",
    "        \n",
    "    for job in jobs:\n",
    "        job.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek-2.tw_raw_txt_kws_tag connected successfully!\n",
      "Reading ./tmp/tag_tw_raw_text-0.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-1.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-2.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-3.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-4.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-5.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-6.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-7.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-8.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-9.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Reading ./tmp/tag_tw_raw_text-10.json... Importing into tweets_ek-2.tw_raw_txt_kws_tag...\n",
      "Done\n",
      "CPU times: user 26min 3s, sys: 44 s, total: 26min 47s\n",
      "Wall time: 33min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Build a new collection for keywords tag on 'text' field of all tweets\n",
    "Register in config:\n",
    "    TW_RAW_TXT_KWS_TAG_COL = 'tw_raw_txt_kws_tag'\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    procedure_name = 'tag_{}_text'.format(TW_RAW_COL)\n",
    "    process_n = multiprocessing.cpu_count() - 1 # set processes number to CPU numbers minus 1\n",
    "    suffix = 'json'\n",
    "    inter_files = utilities.gen_inter_filenames_list(NB_NAME, procedure_name, process_n, suffix)\n",
    "\n",
    "    tw_raw_txt_kws_tag_col = mongodb.initialize(db_name=DB_NAME, collection_name=TW_RAW_TXT_KWS_TAG_COL)\n",
    "    for inter_file in inter_files:\n",
    "        print('Reading {}...'.format(inter_file), end=' ')\n",
    "        lines = open(inter_file).readlines()\n",
    "        parsed_jsons = [json.loads(line) for line in lines]\n",
    "        \n",
    "        print('Importing into {}.{}...'.format(DB_NAME, TW_RAW_TXT_KWS_TAG_COL))\n",
    "        tw_raw_txt_kws_tag_col.insert_many(parsed_jsons)\n",
    "        del lines\n",
    "        del parsed_jsons\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## LEGACY Build new collections of unique users information (multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "_Step 1_ Get a set of unique user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "unique_user_ids_shl = os.path.join('data', 'unique_user_ids.db')\n",
    "unique_user_ids_key = 'unique_user_ids'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying MongoDB for unique user ids...\n",
      "Building unique user ids set from list...\n",
      "Writing out user ids set to shelve data/unique_user_ids.db...\n",
      "Done\n",
      "CPU times: user 1min, sys: 1.12 s, total: 1min 1s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "if 0 == 1:\n",
    "    print('Querying MongoDB for unique user ids...')\n",
    "    unique_user_ids_int64_list = []\n",
    "    cursor = updated_data.find(projection={'_id': 0, 'user.id': 1})\n",
    "    for document in cursor:\n",
    "        user_id_int64 = int(document['user']['id'])\n",
    "        unique_user_ids_int64_list.append(user_id_int64)\n",
    "    \n",
    "    print('Building unique user ids set from list...')\n",
    "    unique_user_ids_int64_set = set(unique_user_ids_int64_list)\n",
    "    \n",
    "    # write out to shelve\n",
    "    print('Writing out user ids set to shelve {} size {}'.format(unique_user_ids_shl, len(unique_user_ids_int64_set)))\n",
    "    with shelve.open(unique_user_ids_shl) as s:\n",
    "        s[unique_user_ids_key] = unique_user_ids_int64_set # store data at key (overwrites old data if using an existing key)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "_Step 2_ For each unique user id in the set, (multiprocessing) query database and write out to intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "MongoDB on localhost:27017/tweets_ek.c2 connected successfully!\n",
      "Process0/11 querying users 0 to 76788...\n",
      "Process1/11 querying users 76788 to 153576...\n",
      "Process2/11 querying users 153576 to 230364...\n",
      "Process3/11 querying users 230364 to 307152...\n",
      "Process4/11 querying users 307152 to 383940...\n",
      "Process5/11 querying users 383940 to 460728...\n",
      "Process6/11 querying users 460728 to 537516...\n",
      "Process7/11 querying users 537516 to 614304...\n",
      "Process8/11 querying users 614304 to 691092...\n",
      "Process9/11 querying users 691092 to 767880...\n",
      "Process10/11 querying users 767880 to 844675...\n",
      "Process0/11 Done\n",
      "Process10/11 Done\n",
      "Process9/11 Done\n",
      "Process8/11 Done\n",
      "Process7/11 Done\n",
      "Process6/11 Done\n",
      "Process5/11 Done\n",
      "Process4/11 Done\n",
      "Process2/11 Done\n",
      "Process3/11 Done\n",
      "Process1/11 Done\n",
      "CPU times: user 60 ms, sys: 116 ms, total: 176 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "inter_files = []\n",
    "if 0 == 1:\n",
    "    # generate intermediate filenames\n",
    "    procedure_name = 'get_{}_unique_user_ids'.format(UPDATED_COL)\n",
    "    \n",
    "    process_n = multiprocessing.cpu_count() - 1 # set processes number to CPU numbers minus 1\n",
    "    suffix = 'json'\n",
    "    inter_files = utilities.gen_inter_filenames_list(procedure_name, process_n, suffix)\n",
    "    \n",
    "    jobs = []\n",
    "    for batch_i in range(process_n):\n",
    "        p = multiprocessing.Process(target=multiprocessing_workers.worker_get_unique_user,\n",
    "                                    args=(DB_NAME, UPDATED_COL,\n",
    "                                          batch_i, process_n, inter_files[batch_i],\n",
    "                                          unique_user_ids_shl, unique_user_ids_key),\n",
    "                                    name='Process-{}/{}'.format(batch_i, process_n))\n",
    "        jobs.append(p)\n",
    "    \n",
    "    for job in jobs:\n",
    "        job.start()\n",
    "        \n",
    "    for job in jobs:\n",
    "        job.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "_Step 3_ Import all unique user data into database new collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2_users connected successfully!\n",
      "Reading inter/get_c2_unique_user_ids-0.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-1.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-2.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-3.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-4.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-5.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-6.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-7.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-8.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-9.json... Importing into tweets_ek.c2_users...\n",
      "Reading inter/get_c2_unique_user_ids-10.json... Importing into tweets_ek.c2_users...\n",
      "Done\n",
      "CPU times: user 2min 29s, sys: 6.82 s, total: 2min 36s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section generate a new collection for all users information.\n",
    "Register USERS_COL = 'c2_users' in config if first time.\n",
    "\"\"\"\n",
    "if 0 == 1:\n",
    "    user_col = mongodb.initialize(db_name=DB_NAME, collection_name=USERS_COL)\n",
    "    for inter_file in inter_files:\n",
    "        print('Reading {}...'.format(inter_file), end=' ')\n",
    "        parsed_jsons = []\n",
    "        with open(inter_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parsed_jsons.append(json.loads(line))\n",
    "        print('Importing into {}.{}...'.format(DB_NAME, USERS_COL))\n",
    "        user_col.insert_many(parsed_jsons)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Check the new collection size and print a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB on localhost:27017/tweets_ek.c2_users connected successfully!\n",
      "Collection c2_users size: 844675\n",
      "Sample document:\n",
      "{'_id': ObjectId('58fed783fe57a10b2393c51e'),\n",
      " 'contributors_enabled': False,\n",
      " 'created_at': 'Tue Mar 21 20:50:14 +0000 2006',\n",
      " 'default_profile': False,\n",
      " 'default_profile_image': False,\n",
      " 'description': '',\n",
      " 'entities': {'description': {'urls': []}},\n",
      " 'favourites_count': 16835,\n",
      " 'follow_request_sent': False,\n",
      " 'followers_count': 4028041,\n",
      " 'following': False,\n",
      " 'friends_count': 2677,\n",
      " 'geo_enabled': True,\n",
      " 'has_extended_profile': True,\n",
      " 'id': 12,\n",
      " 'id_str': '12',\n",
      " 'is_translation_enabled': False,\n",
      " 'is_translator': False,\n",
      " 'lang': 'en',\n",
      " 'listed_count': 27165,\n",
      " 'location': 'California, USA',\n",
      " 'name': 'jack',\n",
      " 'notifications': False,\n",
      " 'profile_background_color': 'EBEBEB',\n",
      " 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme7/bg.gif',\n",
      " 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme7/bg.gif',\n",
      " 'profile_background_tile': False,\n",
      " 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/12/1483046077',\n",
      " 'profile_image_url': 'http://pbs.twimg.com/profile_images/839863609345794048/mkpdB9Tf_normal.jpg',\n",
      " 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/839863609345794048/mkpdB9Tf_normal.jpg',\n",
      " 'profile_link_color': '990000',\n",
      " 'profile_sidebar_border_color': 'DFDFDF',\n",
      " 'profile_sidebar_fill_color': 'F3F3F3',\n",
      " 'profile_text_color': '333333',\n",
      " 'profile_use_background_image': True,\n",
      " 'protected': False,\n",
      " 'screen_name': 'jack',\n",
      " 'statuses_count': 21755,\n",
      " 'time_zone': 'Pacific Time (US & Canada)',\n",
      " 'translator_type': 'regular',\n",
      " 'url': None,\n",
      " 'utc_offset': -25200,\n",
      " 'verified': True}\n"
     ]
    }
   ],
   "source": [
    "if 0 == 1:\n",
    "    user_col = mongodb.initialize(db_name=DB_NAME, collection_name=USERS_COL)\n",
    "    print('Collection {} size: {}'.format(USERS_COL, user_col.count()))\n",
    "    print('Sample document:')\n",
    "    pprint(user_col.find_one())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
